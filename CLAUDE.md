# CLAUDE.md
<!-- Generated by Claude Conductor v1.1.2 -->

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Critical Context (Read First)
- **Tech Stack**: Python 3.11, PyTorch 2.5.1, CUDA 12.4, Docker, Gradio
- **Main File**: Dockerfile (not yet created), inference scripts (planned)
- **Core Mechanic**: Containerize HuMo AI model for RunPod GPU deployment
- **Key Integration**: Hugging Face models, FFmpeg processing, NVIDIA CUDA
- **Platform Support**: RunPod GPU cloud, Docker containers
- **DO NOT**: Hardcode API keys, expose model weights, modify model architectures

## Session Startup Checklist
**IMPORTANT**: At the start of each session, check these items:
1. **Check TASKS.md** - Look for any IN_PROGRESS or BLOCKED tasks from previous sessions
2. **Review recent JOURNAL.md entries** - Scan last 2-3 entries for context
3. **If resuming work**: Load the current task context from TASKS.md before proceeding

## Table of Contents
1. [Architecture](ARCHITECTURE.md) - Tech stack, folder structure, infrastructure
2. [Runtime Config](CONFIG.md) - Environment variables, feature flags, model settings
3. [Build & Release](BUILD.md) - Docker build, deployment, CI/CD pipeline
4. [Testing Guide](TEST.md) - Model validation, integration testing, coverage
5. [Operational Playbooks](PLAYBOOKS/DEPLOY.md) - RunPod deployment, monitoring
6. [Contributing](CONTRIBUTING.md) - Development setup, Docker conventions
7. [Error Ledger](ERRORS.md) - Critical P0/P1 error tracking
8. [Task Management](TASKS.md) - Active tasks, phase tracking, context preservation

## Quick Reference
**Dockerfile**: `Dockerfile` - Container specification with NVIDIA CUDA support
**Model Config**: `generate.yaml` - Inference parameters and mode settings
**Inference Scripts**: `scripts/infer_ta.sh`, `scripts/infer_tia.sh` - Text-to-video generation
**Environment**: `.env` - Docker Hub credentials and configuration
**Base Image**: `nvidia/cuda:12.5.1-cudnn-devel-ubuntu22.04` - CUDA runtime environment
**Main Model**: `Wan-AI/Wan2.1-T2V-1.3B` - Text-to-video generation
**HuMo Model**: `bytedance-research/HuMo` - Human motion generation
**Audio Models**: `openai/whisper-large-v3`, `huangjackson/Kim_Vocal_2` - Speech processing
**Web Interface**: Gradio app on port 7860 - User interaction frontend
**Weights Directory**: `./weights/` - Local model storage

## Current State
- [x] Documentation framework established
- [ ] Docker container implementation
- [ ] Model download and setup
- [ ] Inference scripts creation
- [ ] Gradio web interface
- [ ] CI/CD pipeline setup
- [ ] RunPod deployment testing

## Development Workflow
1. **Setup Environment**: Create Dockerfile with CUDA support and Python dependencies
2. **Model Integration**: Configure Hugging Face model downloads and local storage
3. **Inference Pipeline**: Implement text-to-video generation with audio support
4. **Web Interface**: Create Gradio UI for user interaction and file uploads
5. **Container Build**: Test Docker image locally and push to registry
6. **Deployment**: Deploy to RunPod with GPU configuration and monitoring

## Task Templates
### 1. Docker Container Setup
1. Create `Dockerfile` with `nvidia/cuda:12.5.1-cudnn-devel-ubuntu22.04` base image
2. Install Python 3.11, PyTorch 2.5.1, and dependencies via conda
3. Add model download scripts and weights directory setup
4. Test container build and GPU detection
5. Update `BUILD.md` with build instructions

### 2. Model Integration
1. Configure `generate.yaml` with inference parameters
2. Create model download scripts for Hugging Face integration
3. Implement `scripts/infer_ta.sh` for text+audio mode
4. Implement `scripts/infer_tia.sh` for text+image+audio mode
5. Test model loading and basic inference

### 3. Web Interface Development
1. Create Gradio application with file upload interface
2. Implement text prompt input and parameter controls
3. Add video output display and download functionality
4. Integrate with inference scripts for processing
5. Test user workflow end-to-end

## Anti-Patterns (Avoid These)
❌ **Don't hardcode API keys or credentials** - Use environment variables and Docker secrets
❌ **Don't commit model weights to git** - Models should be downloaded at runtime, not stored in repo
❌ **Don't use unverified base images** - Stick to official NVIDIA CUDA images for security
❌ **Don't ignore GPU memory limits** - Configure proper batch sizes and memory management
❌ **Don't skip model validation** - Always test model loading and inference before deployment
❌ **Don't expose internal ports publicly** - Use proper Docker networking and security groups

## Journal Update Requirements
**IMPORTANT**: Update JOURNAL.md regularly throughout our work sessions:
- After completing any significant feature or fix
- When encountering and resolving errors
- At the end of each work session
- When making architectural decisions
- Format: What/Why/How/Issues/Result structure

## Task Management Integration
**How TASKS.md and JOURNAL.md work together**:
1. **Active Work**: TASKS.md tracks current/incomplete tasks with full context
2. **Completed Work**: When tasks complete, they generate JOURNAL.md entries with `|TASK:ID|` tags
3. **History**: JOURNAL.md preserves complete task history even if Claude Code is reinstalled
4. **Context Recovery**: Search JOURNAL.md for `|TASK:` to see all completed tasks over time
5. **Clean Handoffs**: TASKS.md always shows what needs to be resumed or completed

## Version History
- **v1.0.0** - Initial release
- **v1.1.0** - Feature added (see JOURNAL.md YYYY-MM-DD)  
[Link major versions to journal entries]